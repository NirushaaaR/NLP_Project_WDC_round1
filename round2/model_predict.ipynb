{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import deepcut\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['เมื่อไหร่ลูกของฉันจะเป็นผู้เป็นคนอย่างเขาซักที', 'เขาโดนแตนต่อยที่ขา 4 ตัว', 'ดูผู้ชายคนนั้นสิ เขาแกะกุ้งให้แฟนกินด้วย ดีจังเลย', 'ทำไมเขาของไอ่ทุยถึงงอได้', 'ควายของเขานั้นฉลาดมาก', 'เขาของควายนั้นแข็งมาก', 'เขาแม้ว่าจะสูงเพียงใด ไม่ว่าใคร ถ้าตั้งใจเราจะพิชิตมันได้', 'ปัญหาแม้ว่าจะใหญ่เพียงใด ไม่ว่าใคร ถ้าตั้งใจเขาจะพิชิตมันได้', 'การปีนเขาเป็นกิจกรรมยามว่างที่ฉันทำเป็นประจำทุกอาทิตย์', 'เขาปีนกำแพงที่สูง เพื่อเข้าไปลักพาตัวเด็ก']\n['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n"
    }
   ],
   "source": [
    "# Read Input and sequence number\n",
    "read_input = [i.replace(\"\\n\", '') for i in open(\"input.txt\", encoding=\"utf8\")]\n",
    "\n",
    "input_sentence = [r.split(\"::\")[1] for r in  read_input]\n",
    "input_sentence_num = [r.split(\"::\")[0] for r in  read_input]\n",
    "\n",
    "print(input_sentence[:10])\n",
    "print(input_sentence_num[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create word vector and Sentence vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [[w for w in deepcut.tokenize(s) if w != ' '] for s in input_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[['รู้', 'ตัว', 'ดี', 'ว่า', 'ฉัน', 'ไม่', 'อาจ', 'จะ', 'ไป', 'เทียบ', 'เขา', 'ได้'], ['เขา', 'ไม่', 'รัก', 'เธอ', 'แล้ว', 'อย่าง', 'เสียใจ', 'ไป'], ['มอง', 'หา', 'ร้าน', 'อาหาร', 'ดี', 'ๆ', 'บรรยากาศ', 'วิว', 'เขา', 'กัน', 'อยู่', 'ใช่', 'ไหม'], ['โรงแรม', 'ใน', 'นครราชสีมา', 'ซึ่ง', 'อยู่', 'ใกล้', 'อุทยานแห่งชาติเขาใหญ่'], ['เขตรักษา', 'พันธุ์สัตว์ป่า', 'เขาอ่างฤๅไน', 'พบ', 'วัว', 'แดง', 'ใน', 'พื้นที่', 'หลาย', 'จุด'], ['เนิน', 'เขา', 'แห่ง', 'พระพุทธเจ้า', 'ถือ', 'เป็น', 'สถานที่', 'ท่องเที่ยว', 'แห่ง', 'ใหม่', 'ของ', 'เมืองซัปโปโร'], ['มี', 'ใคร', 'อยาก', 'เสีย', 'พลังงาน', 'ไป', 'กับ', 'การ', 'เดิน', 'ขึ้น', 'เขา', 'ที่', 'อยู่', 'ระหว่าง', 'ทาง'], ['สำหรับ', 'เส้นทาง', 'ลาด', 'ชัน', 'ขึ้น', 'เขา', 'หรือ', 'ขึ้น', 'เนิน', 'ต้อง', 'ใช้', 'ตำแหน่ง', 'เกียร์', 'ที่', 'เหมาะสม'], ['ที่', 'ราบ', 'เชิง', 'เขา', 'สี', 'เขียว', 'ขจี', 'ที่', 'ซึ่ง', 'เกิด', 'ขึ้น', 'ด้วย', 'ความ', 'ตั้งใจ', 'ของ', 'คุณเปิ้ล'], ['ตรึง', 'พื้นที่', 'เชิง', 'เขา', 'หลัง', 'เกิด', 'ปะทะ', 'กับ', 'กลุ่ม', 'ชาย', 'ต้อง', 'สงสัย', 'ใน', 'พื้นที่', 'อำเภอรือเสาะ'], ['แมว', 'บอก', 'ว่า', 'จะ', 'ไป', 'พิชิต', 'ยอดเขาหิมาลัย'], ['กาว', 'บอก', 'ว่า', 'จะ', 'ขึ้น', 'ไป', 'เล่น', 'ว่าว', 'บน', 'ยอด', 'เขา'], ['สมอร', 'จะ', 'ไป', 'ตั้ง', 'แคมป์', 'กับ', 'เพื่อน', 'ๆ', 'บน', 'เขา'], ['กิน', 'ข้าว', 'กับ', 'เรา', 'อร่อย', 'เหมือน', 'กิน', 'กับ', 'เขา', 'ไหม'], ['มี', 'การ', 'แถลง', 'ข่าว', 'ว่า', 'มี', 'คน', 'หย่อน', 'เขา', 'ควาย', 'ลง', 'ใน', 'หีบ', 'เลือกตั้ง'], ['พ่อ', 'ทำ', 'ด้าม', 'มีด', 'จาก', 'เขา', 'เก้ง'], ['กิ่ง', 'ไม้', 'คล้าย', 'เขากวาง'], ['เขา', 'ไม่', 'ใช่', 'ของ', 'นาย', 'ทุน', 'ที่', 'จะ', 'มา', 'ยึดครอง', 'เป็น', 'ของ', 'ตัว', 'เอง'], ['เห็ด', 'ถอบ', 'บน', 'เขา', 'นั้น', 'ไม่', 'ได้', 'งอก', 'เพราะ', 'การ', 'เผา', 'ป่า'], ['ควัน', 'ไฟ', 'จาก', 'เขา', 'ด้าน', 'นู้น', 'ทำ', 'ให้', 'ฉัน', 'ใกล้', 'ตาย'], ['ทุก', 'คน', 'ใกล้', 'ตาย', 'เพราะ', 'ควัน', 'ที่', 'พัด', 'มา', 'จาก', 'เขา'], ['เขา', 'ไป', 'กิน', 'ก๋วยเตี๋ยว', 'แต่', 'ฉัน', 'จะ', 'ไป', 'กิน', 'ข้าว', 'หมู', 'ทอด'], ['หัวใจ', 'เธอ', 'ให้', 'เขา', 'ไป', 'หมด', 'แล้ว'], ['เซคิโระ', 'มี', 'มินิบอส', 'ตัว', 'หนึ่ง', 'ชื่อ', 'วัว', 'ไฟ', 'ซึ่ง', 'มี', 'เขา', 'ที่', 'ติด', 'ไฟ', 'อยู่', 'สู้', 'ยาก', 'มาก', 'ๆ', 'สกิล', 'ก็'], ['หมา', 'ตัว', 'นั้น', 'ชอบ', 'เขา', 'มาก'], ['นกเอี้ยง', 'ปินมา', 'เกาะ', 'บน', 'เขา', 'ควาย'], ['ปี', '2017', 'แต่', 'ตั้งแต่', 'ตอน', 'นั้น', 'มา', 'ก็', 'ไม่', 'เห็น', 'เขา', 'ทำ', 'บาร์บี้', 'เรื่อง', 'ใหม่', 'มา', 'อีก', 'เลย'], ['10', 'ที่', 'เที่ยวเชียงราย', '“', 'ดินแดน', 'แห่ง', 'ขุนเขา', '”', 'ที่', 'ไป', 'เมื่อ', 'ไหร่', 'ก็', 'หลง', 'รัก'], ['นอ', 'เป็น', 'เขา', 'ที่', 'ไม่', 'มี', 'กระดูก', 'เป็น', 'แกน', 'กลาง'], ['พวก', 'เขา', 'บอก', 'ว่า', 'Project', 'นี้', 'ยาก', 'มากกกกกก']]\n"
    }
   ],
   "source": [
    "# Extracted only the word between \"เขา\" 5 word\n",
    "def split_from_kau(words):\n",
    "    q = []\n",
    "    flag_found_kau = False\n",
    "    after_found = 0\n",
    "    for w in words:\n",
    "        q.append(w)\n",
    "        if \"เขา\" in w:\n",
    "            flag_found_kau = True\n",
    "            after_found = len(q)\n",
    "        elif flag_found_kau:\n",
    "            if len(q) >= 21:\n",
    "                break\n",
    "        else:\n",
    "            if len(q) > 10:\n",
    "                q.pop(0)\n",
    "    return q\n",
    "# print(index_of_kau(words[0]))\n",
    "words_split = [split_from_kau(w) for w in words]\n",
    "print(words_split[-30:])\n",
    "max_sentence_length = 21 # should not be larger than 21 word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set([w for s in words_split for w in s])\n",
    "pretrained_word_vec_file = open('cc.th.300.vec', 'r',encoding = 'utf-8-sig')\n",
    "count = 0\n",
    "vocab_vec = {}\n",
    "for line in pretrained_word_vec_file:\n",
    "    if count > 0:\n",
    "        line = line.split()\n",
    "        if(line[0] in vocab): \n",
    "            vocab_vec[line[0]] = line[1:]\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the initial with 0\n",
    "word_vector_length = 300\n",
    "word_vectors = np.zeros((len(words_split),max_sentence_length,word_vector_length))\n",
    "sample_count = 0\n",
    "for s in words_split:\n",
    "    word_count = 0\n",
    "    for w in s:\n",
    "        try:\n",
    "            word_vectors[sample_count,max_sentence_length-word_count-1,:] = vocab_vec[w]\n",
    "            word_count = word_count+1\n",
    "        except:\n",
    "            pass\n",
    "    sample_count = sample_count+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dictlabel prepare to convert it back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'H': 0, 'P': 1, 'M': 2}\n{0: 'H', 1: 'P', 2: 'M'}\n"
    }
   ],
   "source": [
    "dict_label = {\"H\":0,\"P\":1,\"M\":2}\n",
    "dict_label_reverse = {v:k for k,v in dict_label.items()}\n",
    "print(dict_label)\n",
    "print(dict_label_reverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model_18\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_21 (InputLayer)        [(None, 21, 300)]         0         \n_________________________________________________________________\ngru_4 (GRU)                  (None, 30)                29790     \n_________________________________________________________________\ndense_35 (Dense)             (None, 5)                 155       \n_________________________________________________________________\ndense_36 (Dense)             (None, 3)                 18        \n=================================================================\nTotal params: 29,963\nTrainable params: 29,963\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "#Load model\n",
    "model = load_model('model2.h5')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[9.5229901e-05 9.9977082e-01 1.3397919e-04]\n [2.4632862e-04 9.9959904e-01 1.5470682e-04]\n [1.8913430e-04 9.9974555e-01 6.5299326e-05]\n [9.9973232e-01 9.1851936e-05 1.7579696e-04]\n [9.9300051e-01 6.2333634e-03 7.6617114e-04]\n [9.9970478e-01 6.1147097e-05 2.3401437e-04]\n [3.1039606e-05 9.9831593e-01 1.6529926e-03]\n [1.9171915e-04 9.9974233e-01 6.5945213e-05]\n [4.3890649e-04 3.0997166e-04 9.9925107e-01]\n [4.6851453e-03 8.1262266e-04 9.9450219e-01]]\n"
    }
   ],
   "source": [
    "y_pred = model.predict(word_vectors)\n",
    "print(y_pred[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['P', 'P', 'P', 'H', 'H', 'H', 'P', 'P', 'M', 'M', 'H', 'P', 'M', 'M', 'P', 'M', 'M', 'M', 'M', 'P', 'H', 'H', 'H', 'M', 'M', 'H', 'P', 'P', 'M', 'H', 'M', 'M', 'H', 'P', 'M', 'M', 'M', 'M', 'M', 'M', 'P', 'P', 'M', 'P', 'H', 'H', 'M', 'H', 'H', 'P', 'M', 'M', 'M', 'P', 'M', 'M', 'P', 'P', 'H', 'H', 'M', 'M', 'H', 'M', 'P', 'M', 'M', 'M', 'H', 'M', 'M', 'P', 'H', 'H', 'H', 'M', 'P', 'P', 'H', 'M', 'M', 'P', 'P', 'P', 'M', 'H', 'H', 'M', 'H', 'P', 'H', 'H', 'H', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'M', 'H', 'M', 'H', 'H', 'M', 'M', 'M', 'M', 'P', 'M', 'P', 'H', 'M', 'H', 'H', 'H', 'H', 'P', 'M', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'H', 'M', 'P', 'M', 'M', 'H', 'M', 'M', 'H', 'M', 'P', 'M', 'M', 'M', 'M', 'M', 'P', 'M', 'P', 'M', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'H', 'P', 'P', 'P', 'P', 'P', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'P', 'P', 'M', 'P', 'P', 'M', 'P', 'P', 'M', 'P', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'P', 'P', 'P', 'P', 'P', 'H', 'P', 'H', 'P', 'P', 'P', 'M', 'P', 'M', 'P', 'M', 'P', 'M', 'P', 'H', 'M', 'P', 'H', 'H', 'M', 'P', 'P', 'P', 'P', 'P', 'H', 'P', 'M', 'H', 'P', 'H', 'H', 'P', 'M', 'H', 'P', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'P', 'M', 'H', 'M', 'H', 'H', 'M', 'P', 'M', 'M', 'H', 'H', 'H', 'H', 'H', 'H', 'M', 'H', 'M', 'H', 'H', 'H', 'H', 'P', 'P', 'P', 'H', 'M', 'P', 'P', 'P', 'M', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'M', 'H', 'H', 'P', 'M', 'M', 'H', 'H', 'H', 'P', 'H', 'H', 'P', 'M', 'H', 'H', 'H', 'M', 'M', 'M', 'M', 'M', 'M', 'P', 'H', 'P', 'P', 'P', 'P', 'P', 'P', 'H', 'H', 'P', 'P', 'P', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'M', 'P', 'M', 'M', 'P', 'H', 'H', 'H', 'M', 'M', 'M', 'M', 'P', 'P', 'H', 'H', 'H', 'P', 'M', 'H', 'P']\n"
    }
   ],
   "source": [
    "def find_index_of_max(l):\n",
    "    i = (0,l[0])\n",
    "    for y in enumerate(l):\n",
    "        if i[1] < y[1]:\n",
    "            i = y\n",
    "    return i[0]\n",
    "\n",
    "labels = [dict_label_reverse[find_index_of_max(pred)] for pred in y_pred]\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(\"ans.txt\", \"w\", \"utf-8\") as writer:\n",
    "    for res in zip(input_sentence_num, labels):\n",
    "        writer.write(res[0]+\"::\"+res[1]+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('NLP': conda)",
   "language": "python",
   "name": "python37664bitnlpconda01760896482d47d5b516f31bea11e24d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}